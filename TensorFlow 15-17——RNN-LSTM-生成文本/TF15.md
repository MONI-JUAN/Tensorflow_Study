## 一、RNN

### 1.单层神经网络

单层网络的输入是 x，经过变焕 Wx+b 和激活函数f得到输出 y。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913074635557.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)




对于序列形的数据，例如：

- 自然语言处理中的 abcd 连着的单词
- 语音处理中的 abcd 连续声音信号
- 时间序列问题，股票价格之类的



### 2.引入隐状态 h



序列形的数据不太好用原始的神经网络处理。为了处理建模序列问题，RNN引入了隐状态 h(hidden state）的概念，h 可以对序列形的数据提取特征，接着再转换为输出。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913074802317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)
其中：

- 圆圈方块表示向量
- 箭头表示对向量变换
- U、W是参数矩阵
- b是偏置项参数
- f是激活函数，在RNN中通常使用tanh


计算了h1之后，h2 也是一样的，这里U、W、b都是一样的，参数共享：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913082725887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

以此类推计算所有h：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913082746644.png#pic_center)



### 3.输出y



因为 h 是对序列形的数据提取特征，所以到现在都没有输出，输出是通过 h 进行计算，例如 y1：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913082917680.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

这里的 V 和 c 都是新参数。


其他的输出 y 也类似：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913083016348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

这就是 RNN 的结构，输入是 x ，输出是 y ，是等长的。



### 4.限制

因为输入输出序列必须是等长的，所以只有一些问题使用经典的 RNN 结构模型：

- 视频中每一帧的分类
- 输入字符，输出下一个字符的概率，就是 Char RNN



### 5.RNN的数学定义


$$
h_{t} = f(Ux_t+Wh_{t-1}+b)\\
y_t = Softmax(Vh_t+c)
$$

其中，$U,V,W,b,c$ ，均为参数，而 $f$ 表示激活函数，一般为 tanh 函数。





## 二、N VS 1 RNN

### 1.结构图

如果输出是单值而不是序列，就只在最后一个 h 进行输出变换，结构如图：



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913084419377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)



### 2.数学表达

输入为：$x_1,x_2,\cdots,x_t,\cdots,x_T$ 
隐状态为：$h_1,h_2,\cdots,h_t,\cdots,h_T$ 
输出为：$Y$
运算过程为：
$$
h_t=f(Ux_t+Wh_{t-1}+b)
$$
结果$Y$的计算：

$$
Y=Softmax(Vh_r+c)
$$

## 三、1 VS N RNN

### 1.结构图

1 VS N的就是一个输入，一列输出，有两种结构

一种是：



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913093609253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)



另一种是：



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913093645702.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

这两种图等价

### 2.数学表达

数学公式为：
$$
h_t=f(UX+Wh_{t-1}+b)\\
y_t=Softmax(Vh_t+c)
$$

### 3.应用

1 VS N 可以处理的问题有：

- 从图像生成文字（image caption），输入的是图像特征，输出的是一段句子
- 从类别生成语音或者音乐

## 四、LSTM

RNN 的改进版 LSTM ( Long Short-Term Memory，长短期记忆网络）。

LSTM 的输入和输出与 RNN 一样，所以可以无缝替换。



RNN的公式 $h_t=f(Ux_t+Wh_{t-1}+b)$ ，容易梯度爆炸和梯度消失，所以 RNN 很难处理“长程依赖”问题。



### 1.RNN 和 LSTM 的图示

RNN 的图示：



![RNN](https://img-blog.csdnimg.cn/2020091310115644.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)



对应公式：
$$
h_{t} = f(Ux_t+Wh_{t-1}+b)
$$

再看 LSTM 的图示：



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913101313207.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

各个符号的含义：

- 矩形：激活函数
- 圆形：逐点运算（两个矩形进行相加、相乘、等）
- 箭头：指向运算点

### 2.LSTM 的隐状态

LSTM 的隐状态有两部分：

- $h_t$ ：同 RNN
- $C_t$ ：在主干道上传递，避免梯度爆炸和梯度消失

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913103928819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

 $C_t$ 不是完全照搬 $C_{t-1}$ ，而是在$C_{t-1}$ 的基础上选择性的“遗忘”，“记住”一些内容，也就需要用到“遗忘门”和“记忆门”。



### 3.遗忘门

控制遗忘掉 $C_{t-1}$ 的那些部分。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913105832461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

数学公式：

$$
f_t=\sigma (W_f \cdot [h_{t-1},x_t] + b_f)
$$

其中 $\sigma$ 是 Sigmoid 激活函数，输出在0 ~ 1之间。

遗忘门的输入是 $x_t$ 和 $h_{t-1}$

遗忘门的输出和  $C_{t-1}$ 相同形状的矩阵，和 $C_{t-1}$ 逐点相乘，决定遗忘哪些部分。因此全0则全遗忘，全1则全保留。

(打公式好麻烦。。。点个赞吧）

### 4.记忆门

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913110310218.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

数学公式：

$$
i_t=\sigma (W_i \cdot [h_{t-1},x_t] + b_i)\\
\widetilde{C_t}=tanh(W_C \cdot [h_{t-1},x_t] +b_C)
$$

记忆门的输入也是 $x_t$ 和 $h_{t-1}$

记忆门的输出也是 $\widetilde{C_t}$ 和 $i_t$ 逐点相乘，决定记住哪些部分。


### 5.结合“遗忘”“记忆”

$f_t$  是遗忘门输出的（0 ~ 1），$\widetilde{C_t} * i_t$ 是需要记住的新东西。

输出：
$$
C_t=f_t * C_{t-1} + i_t *\widetilde{C_t}
$$


![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913113154708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

### 6.输出门

这里有两个 $h_t$ ，一个是输出门的输出，传递给下一步用的，还有一个输出才是真正这一步的输出，是这几个门输出的函数。



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913114302550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

输出门的输出就是水平方向的那个，是根据$x_t$ 和 $h_{t-1}$ 计算，结果$h_t$ 通过 $o_t * tanh(C_t)$ 得到。

数学公式：

$$
o_t=\sigma (W_o[h_{t-1,x_t}]+b_o) \\
h_t=o_t * tanh(C_t)
$$


## 五、Char RNN
Char RNN 是 N VS N RNN 的经典结构，也就是：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200913124123220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0NDUxOTA5,size_16,color_FFFFFF,t_70#pic_center)

输入是句子中的字母，输出是下一个字母，也就是预测下文。